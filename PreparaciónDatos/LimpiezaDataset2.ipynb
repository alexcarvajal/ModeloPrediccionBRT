{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gZib39dDqz5cBmtULbG7-WZMJVrJhP8P",
      "authorship_tag": "ABX9TyM12blN6Xz6/ALAt7vtcJyP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcarvajal/ModeloPrediccionBRT/blob/main/Preparaci%C3%B3nDatos/LimpiezaDataset2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9TPCgEPf9Ly",
        "outputId": "af9f51bb-7614-47b9-ee7d-1bb5ddad553a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18017 entries, 0 to 18016\n",
            "Data columns (total 10 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Month            18017 non-null  int64  \n",
            " 1   Number_Day       18017 non-null  int64  \n",
            " 2   Day_Week_Number  18017 non-null  int64  \n",
            " 3   Holidays         18017 non-null  float64\n",
            " 4   Hour_Number      18017 non-null  int64  \n",
            " 5   Minute           18017 non-null  int64  \n",
            " 6   Station          18017 non-null  object \n",
            " 7   Station_Access   18017 non-null  object \n",
            " 8   Device           18017 non-null  int64  \n",
            " 9   Inputs           18017 non-null  int64  \n",
            "dtypes: float64(1), int64(7), object(2)\n",
            "memory usage: 1.4+ MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-ea1d55f9318a>:106: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  grouped_data15min['Day'] = pd.to_datetime(grouped_data15min['Day']).copy()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['(05000) Portal Américas', '(07000) Portal Sur',\n",
              "       '(08000) Portal Tunal', '(09000) Cabecera Usme',\n",
              "       '(10000) Portal 20 de Julio', '(06000) Portal Eldorado',\n",
              "       '(02000) Cabecera Autopista Norte', '(02300) Calle 100',\n",
              "       '(03000) Portal Suba', '(04000) Cabecera Calle 80'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import holidays\n",
        "#Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Trabajo de Grado/validacionTroncal20230608.csv')\n",
        "\n",
        "df.columns = ['Station_Access','Day_Group_Type', 'Device', 'Emitter', 'Station','Phase', 'Clearing_Date','Transaction_Date','Peak_Hour','ID_Vehicle','Line','Profile_Name','Card_Number','Operator','Route','Balance_after_transaction','Balance_Before_Transaction','System','Rate_Type','Card_Type','Vehicle_Type','Value','archive']\n",
        "nombres_columnas = df.dtypes\n",
        "#Obtener los valores únicos de cada columna\n",
        "unique_values_day_group_type = df['Day_Group_Type'].unique()\n",
        "unique_values_fase = df['Phase'].unique()\n",
        "unique_values_hora_pico = df['Peak_Hour'].unique()\n",
        "unique_values_sistema = df['System'].unique()\n",
        "unique_values_tipo_tarjeta = df['Card_Type'].unique()\n",
        "unique_values_tipo_tarifa =df[\"Rate_Type\"].unique()\n",
        "unique_values_operador = df [\"Operator\"].unique()\n",
        " # Crear un diccionario con los valores únicos\n",
        "valores_unicos = {\n",
        "     'Day_Group_Type': unique_values_day_group_type,\n",
        "     'Phase': unique_values_fase,\n",
        "     'Peak_Hour': unique_values_hora_pico,\n",
        "     'System': unique_values_sistema,\n",
        "    'Card_Type': unique_values_tipo_tarjeta,\n",
        "     'Rate_Type': unique_values_tipo_tarifa,\n",
        "   \"Operator\": unique_values_operador\n",
        " }\n",
        "\n",
        "\n",
        "\n",
        "Delete6Columns = df.drop(['Day_Group_Type', 'Phase','Peak_Hour','System','Card_Type','Rate_Type','Operator'], axis=1)\n",
        "Delete6Columns\n",
        "\n",
        "#Delete6Columns.to_csv('validacionTroncal20230608Col15.csv', index=False)\n",
        "Delete6Column2= Delete6Columns.drop (['Emitter','Balance_after_transaction','Balance_Before_Transaction','Value','Profile_Name','Card_Number'], axis=1)\n",
        "Delete6Column2\n",
        "\n",
        "#Delete6Column2.to_csv('validacionTroncal20230608Col15Col9.csv', index=False)\n",
        "# Identificando las columnas con valores NaN\n",
        "nan_columns = Delete6Column2.columns[Delete6Column2.isna().any()].tolist()\n",
        "nan_columns\n",
        "\n",
        "Delete3Columns = Delete6Column2.drop (['ID_Vehicle', 'Route', 'Vehicle_Type'], axis=1)\n",
        "Delete3Columns\n",
        "\n",
        "#Delete2Columns.to_csv('validacionTroncal20230608Col15Col7.csv', index=False)\n",
        "Delete2Columns = Delete3Columns.drop (['Line','Clearing_Date','archive'], axis=1)\n",
        "#Delete2Columns.to_csv('validacionTroncal20230608Col15Col4.csv', index=False)\n",
        "\n",
        "# Find the 10 most frequent unique values of 'Estacion_Parada'\n",
        "#top_10_estaciones = Delete2Columns['Station'].value_counts().head(10)\n",
        "\n",
        "estaciones_seleccionadas = ['(02000) Cabecera Autopista Norte', '(02300) Calle 100','(03000) Portal Suba', '(04000) Cabecera Calle 80','(05000) Portal Américas', '(06000) Portal Eldorado','(07000) Portal Sur','(08000) Portal Tunal','(09000) Cabecera Usme','(10000) Portal 20 de Julio']\n",
        "#top_10_estaciones = top_estaciones.sort_values(ascending=False).head(10)\n",
        "#top_10_estaciones\n",
        "\n",
        "# Filtrar el dataset para incluir solo las 10 estaciones con la mayor cantidad de entradas\n",
        "filtered_top_10_estaciones_data =Delete2Columns[Delete2Columns['Station'].isin(estaciones_seleccionadas)].copy()\n",
        "filtered_top_10_estaciones_data\n",
        "\n",
        "\n",
        "reordered_dataset = filtered_top_10_estaciones_data[['Transaction_Date', 'Station', 'Station_Access', 'Device']].copy()\n",
        "reordered_dataset\n",
        "\n",
        "\n",
        "#reordered_dataset.to_csv('validacionTroncal20230608Col15Col410Estaciones.csv', index=False)\n",
        "\n",
        "reordered_dataset['Transaction_Date'] = pd.to_datetime(reordered_dataset['Transaction_Date'])\n",
        "reordered_dataset['Day_Week_Number'] = reordered_dataset['Transaction_Date'].dt.dayofweek + 1\n",
        "\n",
        "co_holidays = holidays.Colombia(years=[2023])\n",
        "reordered_dataset['Holidays'] = reordered_dataset['Transaction_Date'].apply(lambda x: x in co_holidays)\n",
        "reordered_dataset['Holidays'] = reordered_dataset['Holidays'].astype(float)\n",
        "\n",
        "reordered_dataset['Transaction_Date'] = reordered_dataset['Transaction_Date'].astype(str)\n",
        "\n",
        "# Split the 'Fecha_Transaccion' column into 'Fecha' and 'Hora'\n",
        "reordered_dataset['Date'] = reordered_dataset['Transaction_Date'].str.split(' ').str[0]\n",
        "reordered_dataset['Hour'] = reordered_dataset['Transaction_Date'].str.split(' ').str[1]\n",
        "\n",
        "final_dataset = reordered_dataset[['Date', 'Hour', 'Day_Week_Number', 'Holidays','Station', 'Station_Access', 'Device']]\n",
        "#final_dataset.to_csv('DatasetConFechaSeparada', index=False)\n",
        "final_dataset\n",
        "#final_dataset.to_csv('Dataset2only10top.csv', index=False)\n",
        "final_dataset = final_dataset.rename(columns={'Hour': 'Time'})\n",
        "\n",
        "final_dataset['Time'] = pd.to_datetime(final_dataset['Time'], format='%H:%M:%S').dt.time\n",
        "\n",
        "start_time = pd.to_datetime(\"03:00:00\", format='%H:%M:%S').time()\n",
        "end_time = pd.to_datetime(\"23:59:59\", format='%H:%M:%S').time()\n",
        "\n",
        "final_dataset['Range_15_min'] = pd.to_datetime(final_dataset['Date'] + ' ' + final_dataset['Time'].astype(str)).dt.ceil('15T')\n",
        "grouped_data = final_dataset.groupby(['Range_15_min', 'Day_Week_Number', 'Holidays', 'Station', 'Station_Access', 'Device']).size().reset_index(name='Inputs')\n",
        "\n",
        "grouped_data\n",
        "grouped_data['Range_15_min'] = grouped_data['Range_15_min'].astype(str)\n",
        "\n",
        "# Split the 'Fecha_Transaccion' column into 'Fecha' and 'Hora'\n",
        "grouped_data['Day'] = grouped_data['Range_15_min'].str.split(' ').str[0]\n",
        "grouped_data['Hour'] = grouped_data['Range_15_min'].str.split(' ').str[1]\n",
        "\n",
        "grouped_data15min =  grouped_data[['Day', 'Hour', 'Day_Week_Number', 'Holidays','Station', 'Station_Access', 'Device','Inputs']]\n",
        "grouped_data15min\n",
        "#grouped_data15min.to_csv('validacionTroncal20230608Col15Col410EstacionesGroupInputs.csv', index=False)\n",
        "\n",
        "grouped_data15min['Day'] = pd.to_datetime(grouped_data15min['Day']).copy()\n",
        "grouped_data15min['Year'] = grouped_data15min['Day'].dt.year\n",
        "grouped_data15min['Month'] = grouped_data15min['Day'].dt.month\n",
        "grouped_data15min['Number_Day'] = grouped_data15min['Day'].dt.day\n",
        "\n",
        "grouped_data15min.drop('Day', axis=1,inplace=True)\n",
        "\n",
        "grouped_data15min\n",
        "\n",
        "grouped_data15min['Hour'] = pd.to_datetime(grouped_data15min['Hour'], format='%H:%M:%S').dt.time.copy()\n",
        "grouped_data15min['Hour_Number'] = grouped_data15min['Hour'].apply(lambda x: x.hour)\n",
        "grouped_data15min['Minute'] = grouped_data15min['Hour'].apply(lambda x: x.minute)\n",
        "grouped_data15min['Second'] = grouped_data15min['Hour'].apply(lambda x: x.second)\n",
        "grouped_data15min.drop(['Hour','Second'], axis=1,inplace=True)\n",
        "grouped_data15min\n",
        "\n",
        "ordered_columns = [ 'Year', 'Month', 'Number_Day','Day_Week_Number', 'Holidays', 'Hour_Number', 'Minute', 'Station', 'Station_Access', 'Device','Inputs']\n",
        "grouped_data15minOrganized = grouped_data15min[ordered_columns].copy()\n",
        "grouped_data15minOrganized.drop('Year', axis=1,inplace=True)\n",
        "grouped_data15minOrganized.info()\n",
        "unique = grouped_data15minOrganized['Station'].unique()\n",
        "unique\n",
        "#grouped_data15min.to_csv('validacionTroncal20230608Col15Col410EstacionesGroupInputsDaysHours.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dq2U4vHYh3qp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}