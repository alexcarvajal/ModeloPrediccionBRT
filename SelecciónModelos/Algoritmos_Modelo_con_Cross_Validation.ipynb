{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcarvajal/ModeloPrediccionBRT/blob/main/Selecci%C3%B3nModelos/Algoritmos_Modelo_con_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de Datasets\n"
      ],
      "metadata": {
        "id": "UYswKk-LRMTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os, psutil\n",
        "url1 = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1CodificadoV2.csv'\n",
        "data1 = pd.read_csv(url1)\n",
        "#data1 = data1[data1['Inputs'] != 0]\n",
        "data1\n",
        "url2 = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset2CodificadoV2.csv'\n",
        "data2 = pd.read_csv(url2)\n",
        "#data2\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs3zi4OORLoy",
        "outputId": "7b5ea7ea-1c20-469d-d66b-05f6f7dc03bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage: 431.34765625 megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DoYrRgEUyI0"
      },
      "source": [
        "# Modelo Cross-Validation Dataset 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Dataset 1\n",
        "8 Folds"
      ],
      "metadata": {
        "id": "L5Dj1_AgYZPI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6161b6-cc64-4095-a524-e28fdf784559",
        "id": "hXP2OjvNYLk2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE promedio: 623.7936778299004\n",
            "RMSE promedio: 24.670068924406472\n",
            "MAE promedio: 14.658249125608478\n",
            "R2 promedio: 0.3040731969449977\n",
            "MAPE promedio: inf\n",
            "Memory usage: 463.109375 megabytes\n",
            "Tiempo de ejecución: 20:51\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def adjusted_r2_score(y_true, y_pred, n, k):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "\n",
        "X = data1.drop(columns=['Inputs'])\n",
        "y = data1['Inputs']\n",
        "\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "           'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False),\n",
        "           'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "           'R2': 'r2',\n",
        "           'MAPE': make_scorer(mape, greater_is_better=False)}\n",
        "\n",
        "cv_results = cross_validate(rf_regressor, X, y, cv=8, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
        "\n",
        "n, k = X.shape\n",
        "\n",
        "print(f\"MSE promedio: {-cv_results['test_MSE'].mean()}\")\n",
        "print(f\"RMSE promedio: {-cv_results['test_RMSE'].mean()}\")\n",
        "print(f\"MAE promedio: {-cv_results['test_MAE'].mean()}\")\n",
        "print(f\"R2 promedio: {cv_results['test_R2'].mean()}\")\n",
        "#print(f\"R2 ajustado: {r2_adj}\")\n",
        "print(f\"MAPE promedio: {-cv_results['test_MAPE'].mean()}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Calcular minutos y segundos\n",
        "minutes = int(execution_time // 60)\n",
        "seconds = int(execution_time % 60)\n",
        "\n",
        "print(f\"Tiempo de ejecución: {minutes:02d}:{seconds:02d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE promedio: 492.63131760012857\n",
        "RMSE promedio: 22.178577349583172\n",
        "MAE promedio: 13.027049772057994\n",
        "R2 promedio: 0.4699380220836603\n",
        "MAPE promedio: inf\n",
        "Memory usage: 603.79296875 megabytes"
      ],
      "metadata": {
        "id": "iqydkBqtAv5Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHvF_GN1U65e"
      },
      "source": [
        "## Random Forest Dataset 1\n",
        "5 Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM_77zdDF9WK",
        "outputId": "617a4eaa-5e08-41d1-a2ba-21852517e747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE promedio: 633.55971416802\n",
            "RMSE promedio: 25.145050935652186\n",
            "MAE promedio: 14.843976479020665\n",
            "R2 promedio: 0.3425452692695612\n",
            "MAPE promedio: inf\n",
            "Memory usage: 499.8671875 megabytes\n",
            "Tiempo de ejecución: 12:54\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "start_time = time.time()\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def adjusted_r2_score(y_true, y_pred, n, k):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "X = data1.drop(columns=['Inputs'])\n",
        "y = data1['Inputs']\n",
        "\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "           'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False),\n",
        "           'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "           'R2': 'r2',\n",
        "           'MAPE': make_scorer(mape, greater_is_better=False)}\n",
        "\n",
        "cv_results = cross_validate(rf_regressor, X, y, cv=5, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
        "n, k = X.shape\n",
        "\n",
        "print(f\"MSE promedio: {-cv_results['test_MSE'].mean()}\")\n",
        "print(f\"RMSE promedio: {-cv_results['test_RMSE'].mean()}\")\n",
        "print(f\"MAE promedio: {-cv_results['test_MAE'].mean()}\")\n",
        "print(f\"R2 promedio: {cv_results['test_R2'].mean()}\")\n",
        "#print(f\"R2 ajustado: {r2_adj}\")\n",
        "print(f\"MAPE promedio: {-cv_results['test_MAPE'].mean()}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Calcular minutos y segundos\n",
        "minutes = int(execution_time // 60)\n",
        "seconds = int(execution_time % 60)\n",
        "print(f\"Tiempo de ejecución: {minutes:02d}:{seconds:02d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z25_I3fmYcSy"
      },
      "source": [
        "## LSTM  Dataset 1\n",
        "5 Folds\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY-PrZ2fnxcT",
        "outputId": "8c32294e-0b22-4999-c118-3958a2b4c000"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdZUhLBZGUuy",
        "outputId": "c4940b5a-e105-439f-c9e2-2e4290ee6faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 759.9695680066877, R2: 0.22428870184168048, MAPE: 1.4663542938772296e+16, RMSE: 27.567545556445314, mae: 18.372326498267565\n",
            "MSE: 997.5638394010598, R2: 0.303169897687805, MAPE: 1.2297959895419664e+16, RMSE: 31.584234032204417, mae: 21.52933519600301\n",
            "MSE: 414.7387318566887, R2: 0.36426941837725424, MAPE: 8940073575248869.0, RMSE: 20.365135203496408, mae: 12.581230808228232\n",
            "MSE: 597.6131673118693, R2: 0.4966809136023044, MAPE: 8958748139849965.0, RMSE: 24.446127859271893, mae: 15.018311558835228\n",
            "MSE: 393.94674703298836, R2: 0.44804958548016727, MAPE: 8573324148772275.0, RMSE: 19.84809177308963, mae: 12.132187254546103\n",
            "Resumen de métricas:\n",
            "MSE promedio: 632.7664107218587\n",
            "R2 promedio: 0.3672917033978423\n",
            "MAPE promedio: 1.0686729739612612e+16\n",
            "RMSE promedio: 24.762226884901533\n",
            "MAE promedio: 15.92667826317603\n",
            "Memory usage: 1216.2265625 megabytes\n",
            "Tiempo de ejecución: 17:21\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
        "import time\n",
        "start_time = time.time()\n",
        "import os, psutil\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1CodificadoV2.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "features = data.drop(columns=['Inputs', 'Unnamed: 0'])\n",
        "target = data['Inputs']\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "X = np.reshape(features_scaled, (features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
        "y = target.values\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "metrics_summary = {'mse': [], 'r2': [], 'mape': [], 'rmse':[], 'mae':[]}\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=32, verbose=0)\n",
        "\n",
        "    # Predecir\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "    # Calcular métricas\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
        "    #r2_adjusted = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "    # Guardar métricas\n",
        "    metrics_summary['mse'].append(mse)\n",
        "    metrics_summary['r2'].append(r2)\n",
        "    metrics_summary['mape'].append(mape)\n",
        "    metrics_summary['rmse'].append(rmse)\n",
        "    metrics_summary['mae'].append(mae)\n",
        "\n",
        "\n",
        "    print(f'MSE: {mse}, R2: {r2}, MAPE: {mape}, RMSE: {rmse}, mae: {mae}')\n",
        "\n",
        "# Mostrar resumen de métricas\n",
        "print(\"Resumen de métricas:\")\n",
        "print(f\"MSE promedio: {np.mean(metrics_summary['mse'])}\")\n",
        "print(f\"R2 promedio: {np.mean(metrics_summary['r2'])}\")\n",
        "print(f\"MAPE promedio: {np.mean(metrics_summary['mape'])}\")\n",
        "print(f\"RMSE promedio: {np.mean(metrics_summary['rmse'])}\")\n",
        "print(f\"MAE promedio: {np.mean(metrics_summary['mae'])}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Calcular minutos y segundos\n",
        "minutes = int(execution_time // 60)\n",
        "seconds = int(execution_time % 60)\n",
        "\n",
        "# Formatear y mostrar el tiempo de ejecución\n",
        "print(f\"Tiempo de ejecución: {minutes:02d}:{seconds:02d}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM  Dataset 1\n",
        "8 Folds\n"
      ],
      "metadata": {
        "id": "I7xAQoyWwhWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d37723d-8162-4392-e3d4-6bf44638e185",
        "id": "R9djDKQ7wXVn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 940.1421051912728, R2: 0.188502689817176, MAPE: 1.621141401395365e+16, RMSE: 30.661736826071557, mae: 20.92865486712944\n",
            "MSE: 776.4879041003244, R2: 0.22163389976645675, MAPE: 1.5681531744849808e+16, RMSE: 27.86553254650491, mae: 18.501291946688777\n",
            "MSE: 874.1524696559411, R2: 0.31443703840443504, MAPE: 1.4918516068143442e+16, RMSE: 29.566069567258026, mae: 20.221662971177718\n",
            "MSE: 663.9016272573622, R2: 0.4213582198635497, MAPE: 1.020281673966504e+16, RMSE: 25.766288581349123, mae: 16.05123642114253\n",
            "MSE: 277.2512015976359, R2: 0.18146447654953846, MAPE: 8581778694918524.0, RMSE: 16.650861887531104, mae: 10.757447659965678\n",
            "MSE: 606.2499009366376, R2: 0.4951960604532447, MAPE: 9995521935856852.0, RMSE: 24.622142492818078, mae: 15.17841664785093\n",
            "MSE: 609.1374925328059, R2: 0.4664639022248648, MAPE: 1.133253095114336e+16, RMSE: 24.680710940586902, mae: 15.806084830774104\n",
            "MSE: 249.91281477894742, R2: 0.3437849552661113, MAPE: 7423989123554289.0, RMSE: 15.808631021658625, mae: 9.85203507759797\n",
            "Resumen de métricas:\n",
            "MSE promedio: 624.654439506366\n",
            "R2 promedio: 0.3291051552931721\n",
            "MAPE promedio: 1.179351240901062e+16\n",
            "RMSE promedio: 24.45274673297229\n",
            "MAE promedio: 15.912103802790895\n",
            "Memory usage: 1375.59375 megabytes\n",
            "Tiempo de ejecución: 23:13\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
        "import time\n",
        "import os, psutil\n",
        "\n",
        "start_time = time.time()\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "features = data.drop(columns=['Inputs', 'Unnamed: 0'])\n",
        "target = data['Inputs']\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "X = np.reshape(features_scaled, (features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
        "y = target.values\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=8)\n",
        "\n",
        "metrics_summary = {'mse': [], 'r2': [], 'mape': [], 'rmse':[], 'mae':[]}\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=32, verbose=0)\n",
        "\n",
        "    # Predecir\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "    # Calcular métricas\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
        "    #r2_adjusted = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "    # Guardar métricas\n",
        "    metrics_summary['mse'].append(mse)\n",
        "    metrics_summary['r2'].append(r2)\n",
        "    metrics_summary['mape'].append(mape)\n",
        "    metrics_summary['rmse'].append(rmse)\n",
        "    metrics_summary['mae'].append(mae)\n",
        "\n",
        "\n",
        "    print(f'MSE: {mse}, R2: {r2}, MAPE: {mape}, RMSE: {rmse}, mae: {mae}')\n",
        "\n",
        "print(\"Resumen de métricas:\")\n",
        "print(f\"MSE promedio: {np.mean(metrics_summary['mse'])}\")\n",
        "print(f\"R2 promedio: {np.mean(metrics_summary['r2'])}\")\n",
        "print(f\"MAPE promedio: {np.mean(metrics_summary['mape'])}\")\n",
        "print(f\"RMSE promedio: {np.mean(metrics_summary['rmse'])}\")\n",
        "print(f\"MAE promedio: {np.mean(metrics_summary['mae'])}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "minutes = int(execution_time // 60)\n",
        "seconds = int(execution_time % 60)\n",
        "\n",
        "print(f\"Tiempo de ejecución: {minutes:02d}:{seconds:02d}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAXTAjREYmGC"
      },
      "source": [
        "## SVM  Dataset 1\n",
        "70% Entrenamiento - 30% Prueba\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tULxh3TlJgAT",
        "outputId": "b84f338e-5479-455f-bd42-9e799ed30aa3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "\n",
        "# Preprocesamiento básico\n",
        "# Eliminar columna innecesaria y separar características de la variable objetivo\n",
        "X = data.drop(columns=['Inputs', 'Unnamed: 0'])\n",
        "y = data['Inputs']\n",
        "\n",
        "# Crear un pipeline que primero estandariza los datos y luego aplica SVM\n",
        "svm_pipeline = make_pipeline(StandardScaler(), SVC(kernel='linear', random_state=42))\n",
        "\n",
        "# Aplicar validación cruzada\n",
        "cv_scores = cross_val_score(svm_pipeline, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Mostrar los resultados de la validación cruzada\n",
        "print(f\"Accuracy promedio: {cv_scores.mean()}\")\n",
        "print(f\"Desviación estándar: {cv_scores.std()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmYlav2HY4NQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2u4w2VPbCJR"
      },
      "source": [
        "## XGBoost Dataset 1\n",
        "5 Folds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTP4yETfbGlG",
        "outputId": "3532d788-956a-44ac-ee3e-15baf642e9b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': 5.126800489425659,\n",
              " 'score_time': 0.3325015068054199,\n",
              " 'test_rmse': -20.918190008871687,\n",
              " 'test_r2': 0.5352016323592881,\n",
              " 'test_mae': -12.392111442769579}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preparar los datos\n",
        "X = data.drop(columns=['Inputs', 'Unnamed: 0'])  # Asumiendo que 'Outputs' es la variable objetivo\n",
        "y = data['Inputs']\n",
        "\n",
        "# Configurar el modelo XGBoost\n",
        "model = XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Configurar la validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir las métricas\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "scoring = {\n",
        "    'rmse': make_scorer(rmse, greater_is_better=False),\n",
        "    'r2': 'r2',\n",
        "    'mae': 'neg_mean_absolute_error'\n",
        "}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "# Resultados de la validación cruzada\n",
        "cv_results_mean = {metric: np.mean(scores) for metric, scores in cv_results.items()}\n",
        "cv_results_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rQyeLmGjbqQq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIROuvzSk_5J"
      },
      "source": [
        "## XGBoost Dataset 1\n",
        "8 Folds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1531f274-eca7-44d1-fd88-4f2873fd40fc",
        "id": "Qg3PN-XWk_5P"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fit_time': 5.773262560367584, 'score_time': 0.2566419541835785, 'test_rmse': -20.90517851139823, 'test_r2': 0.5357589932300635, 'test_mae': -12.380066040782962}\n",
            "Memory usage: 1571.734375 megabytes\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preparar los datos\n",
        "X = data.drop(columns=['Inputs', 'Unnamed: 0'])  # Asumiendo que 'Outputs' es la variable objetivo\n",
        "y = data['Inputs']\n",
        "\n",
        "# Configurar el modelo XGBoost\n",
        "model = XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Configurar la validación cruzada\n",
        "kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir las métricas\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "scoring = {\n",
        "    'rmse': make_scorer(rmse, greater_is_better=False),\n",
        "    'r2': 'r2',\n",
        "    'mae': 'neg_mean_absolute_error'\n",
        "}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "# Resultados de la validación cruzada\n",
        "cv_results_mean = {metric: np.mean(scores) for metric, scores in cv_results.items()}\n",
        "print(cv_results_mean)\n",
        "\n",
        "process = psutil.Process()\n",
        "\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zZZplr6Dk_5Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo con Cross Validation Dataset 2"
      ],
      "metadata": {
        "id": "2SNf7efzlgja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Dataset 2\n",
        "8 Folds\n"
      ],
      "metadata": {
        "id": "DLoVk-HIlxNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def adjusted_r2_score(y_true, y_pred, n, k):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Cargar el data2set\n",
        "\n",
        "# Eliminar columna innecesaria\n",
        "#data2 = data1.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Separar las características (X) de la variable objetivo (y)\n",
        "X = data2.drop(columns=['Inputs'])\n",
        "y = data2['Inputs']\n",
        "\n",
        "# Inicializar el modelo de Random Forest para regresión\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Definir las métricas para la validación cruzada\n",
        "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "           'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False),\n",
        "           'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "           'R2': 'r2',\n",
        "           'MAPE': make_scorer(mape, greater_is_better=False)}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(rf_regressor, X, y, cv=8, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
        "\n",
        "# Calcular y mostrar los resultados\n",
        "n, k = X.shape\n",
        "#r2_adj = adjusted_r2_score(y, rf_regressor.predict(X), n, k)\n",
        "\n",
        "print(f\"MSE promedio: {-cv_results['test_MSE'].mean()}\")\n",
        "print(f\"RMSE promedio: {-cv_results['test_RMSE'].mean()}\")\n",
        "print(f\"MAE promedio: {-cv_results['test_MAE'].mean()}\")\n",
        "print(f\"R2 promedio: {cv_results['test_R2'].mean()}\")\n",
        "#print(f\"R2 ajustado: {r2_adj}\")\n",
        "print(f\"MAPE promedio: {-cv_results['test_MAPE'].mean()}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbpkClfLl4d4",
        "outputId": "4cec3d9c-2a74-4639-8adb-3b06a49e867d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE promedio: 649.6346699088571\n",
            "RMSE promedio: 25.114538927100185\n",
            "MAE promedio: 16.406322383021568\n",
            "R2 promedio: 0.3926701280790993\n",
            "MAPE promedio: 304.64753800526694\n",
            "Memory usage: 1571.8671875 megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Random Forest Dataset 2\n",
        "5 Folds\n",
        "\n"
      ],
      "metadata": {
        "id": "J58vQFXvmO-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def adjusted_r2_score(y_true, y_pred, n, k):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Cargar el data2set\n",
        "\n",
        "# Eliminar columna innecesaria\n",
        "#data2 = data1.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Separar las características (X) de la variable objetivo (y)\n",
        "X = data2.drop(columns=['Inputs'])\n",
        "y = data2['Inputs']\n",
        "\n",
        "# Inicializar el modelo de Random Forest para regresión\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Definir las métricas para la validación cruzada\n",
        "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "           'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False),\n",
        "           'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "           'R2': 'r2',\n",
        "           'MAPE': make_scorer(mape, greater_is_better=False)}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(rf_regressor, X, y, cv=5, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
        "\n",
        "# Calcular y mostrar los resultados\n",
        "n, k = X.shape\n",
        "#r2_adj = adjusted_r2_score(y, rf_regressor.predict(X), n, k)\n",
        "\n",
        "print(f\"MSE promedio: {-cv_results['test_MSE'].mean()}\")\n",
        "print(f\"RMSE promedio: {-cv_results['test_RMSE'].mean()}\")\n",
        "print(f\"MAE promedio: {-cv_results['test_MAE'].mean()}\")\n",
        "print(f\"R2 promedio: {cv_results['test_R2'].mean()}\")\n",
        "#print(f\"R2 ajustado: {r2_adj}\")\n",
        "print(f\"MAPE promedio: {-cv_results['test_MAPE'].mean()}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS4ekcObmLzB",
        "outputId": "2bca14c7-195f-4d82-a4c1-b17de2c0f4dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE promedio: 646.3795911678905\n",
            "RMSE promedio: 25.32384902828786\n",
            "MAE promedio: 16.368998923765567\n",
            "R2 promedio: 0.43544843362919183\n",
            "MAPE promedio: 303.54201299466615\n",
            "Memory usage: 1571.8671875 megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Dataset 2\n",
        "8 Folds"
      ],
      "metadata": {
        "id": "pf5_wSbUnL2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset2Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preprocesamiento\n",
        "features = data.drop(columns=['Inputs'])\n",
        "target = data['Inputs']\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Convertir datos para LSTM\n",
        "X = np.reshape(features_scaled, (features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
        "y = target.values\n",
        "\n",
        "# Definir el modelo LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Callback para detener el entrenamiento\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
        "\n",
        "# TimeSeriesSplit para la validación cruzada\n",
        "tscv = TimeSeriesSplit(n_splits=8)\n",
        "\n",
        "metrics_summary = {'mse': [], 'r2': [], 'mape': [], 'rmse':[], 'mae':[]}\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=32, verbose=0)\n",
        "\n",
        "    # Predecir\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "    # Calcular métricas\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
        "    #r2_adjusted = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "    # Guardar métricas\n",
        "    metrics_summary['mse'].append(mse)\n",
        "    metrics_summary['r2'].append(r2)\n",
        "    metrics_summary['mape'].append(mape)\n",
        "    metrics_summary['rmse'].append(rmse)\n",
        "    metrics_summary['mae'].append(mae)\n",
        "\n",
        "\n",
        "    print(f'MSE: {mse}, R2: {r2}, MAPE: {mape}, RMSE: {rmse}, mae: {mae}')\n",
        "\n",
        "# Mostrar resumen de métricas\n",
        "print(\"Resumen de métricas:\")\n",
        "print(f\"MSE promedio: {np.mean(metrics_summary['mse'])}\")\n",
        "print(f\"R2 promedio: {np.mean(metrics_summary['r2'])}\")\n",
        "print(f\"MAPE promedio: {np.mean(metrics_summary['mape'])}\")\n",
        "print(f\"RMSE promedio: {np.mean(metrics_summary['rmse'])}\")\n",
        "print(f\"MAE promedio: {np.mean(metrics_summary['mae'])}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6xA2Sm4nRRo",
        "outputId": "81731a88-d371-4120-bce3-a90ed1cea7cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1509.8627082156665, R2: 0.04925655014555008, MAPE: 6.843925452963294, RMSE: 38.85695186470069, mae: 29.911813829036724\n",
            "MSE: 978.3743546928735, R2: 0.08353020829779823, MAPE: 4.746965843468148, RMSE: 31.27897624112518, mae: 21.755888534263597\n",
            "MSE: 1633.0417389397908, R2: 0.18269771996637507, MAPE: 3.216678847415221, RMSE: 40.41091113721381, mae: 26.54005676663839\n",
            "MSE: 926.4140202524696, R2: 0.39837963392138476, MAPE: 3.5266248726585463, RMSE: 30.437050124025976, mae: 20.921098292789882\n",
            "MSE: 439.974481415117, R2: 0.22408236420812078, MAPE: 2.8182892538388264, RMSE: 20.97556867918286, mae: 13.908274322146434\n",
            "MSE: 778.9202268910317, R2: 0.4810591390468827, MAPE: 3.448292894257626, RMSE: 27.909142353197307, mae: 18.295042164421606\n",
            "MSE: 845.1343945345525, R2: 0.44461592842738273, MAPE: 3.3816545608914876, RMSE: 29.071195271858922, mae: 19.570107286054338\n",
            "MSE: 403.13200356084235, R2: 0.3008240155152134, MAPE: 2.3315517198792746, RMSE: 20.078147413564885, mae: 12.991373276766728\n",
            "Resumen de métricas:\n",
            "MSE promedio: 939.3567410627929\n",
            "R2 promedio: 0.27055569494108844\n",
            "MAPE promedio: 3.7892479306715527\n",
            "RMSE promedio: 29.877242885608705\n",
            "MAE promedio: 20.486706809014713\n",
            "Memory usage: 1540.125 megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Dataset 2\n",
        "5 Folds"
      ],
      "metadata": {
        "id": "p_nL2sL0nte3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset2Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preprocesamiento\n",
        "features = data.drop(columns=['Inputs'])\n",
        "target = data['Inputs']\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Convertir datos para LSTM\n",
        "X = np.reshape(features_scaled, (features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
        "y = target.values\n",
        "\n",
        "# Definir el modelo LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Callback para detener el entrenamiento\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
        "\n",
        "# TimeSeriesSplit para la validación cruzada\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "metrics_summary = {'mse': [], 'r2': [], 'mape': [], 'rmse':[], 'mae':[]}\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=32, verbose=0)\n",
        "\n",
        "    # Predecir\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "    # Calcular métricas\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
        "    #r2_adjusted = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "    # Guardar métricas\n",
        "    metrics_summary['mse'].append(mse)\n",
        "    metrics_summary['r2'].append(r2)\n",
        "    metrics_summary['mape'].append(mape)\n",
        "    metrics_summary['rmse'].append(rmse)\n",
        "    metrics_summary['mae'].append(mae)\n",
        "\n",
        "\n",
        "    print(f'MSE: {mse}, R2: {r2}, MAPE: {mape}, RMSE: {rmse}, mae: {mae}')\n",
        "\n",
        "# Mostrar resumen de métricas\n",
        "print(\"Resumen de métricas:\")\n",
        "print(f\"MSE promedio: {np.mean(metrics_summary['mse'])}\")\n",
        "print(f\"R2 promedio: {np.mean(metrics_summary['r2'])}\")\n",
        "print(f\"MAPE promedio: {np.mean(metrics_summary['mape'])}\")\n",
        "print(f\"RMSE promedio: {np.mean(metrics_summary['rmse'])}\")\n",
        "print(f\"MAE promedio: {np.mean(metrics_summary['mae'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_reZyEblnw8l",
        "outputId": "7a09c742-efd9-4df2-fa08-548cd370b123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1029.5344746469839, R2: 0.17427278451203587, MAPE: 4.907363844899179, RMSE: 32.0863596353183, mae: 22.517256532835674\n",
            "MSE: 1545.291833530455, R2: 0.17342168152126491, MAPE: 3.985718670883746, RMSE: 39.31020012071237, mae: 27.47105021331767\n",
            "MSE: 611.2669415652601, R2: 0.32194574390483577, MAPE: 3.100214708269596, RMSE: 24.723813248875267, mae: 16.75091179655995\n",
            "MSE: 858.4493954616884, R2: 0.43657615965916086, MAPE: 3.7051928143097785, RMSE: 29.29930708159646, mae: 19.84734411210094\n",
            "MSE: 550.3542724610744, R2: 0.3962936449700839, MAPE: 2.638880573534517, RMSE: 23.45963069745716, mae: 14.98757836204879\n",
            "Resumen de métricas:\n",
            "MSE promedio: 918.9793835330923\n",
            "R2 promedio: 0.3005020029134763\n",
            "MAPE promedio: 3.667474122379364\n",
            "RMSE promedio: 29.775862156791913\n",
            "MAE promedio: 20.314828203372606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "orM2Optmqgt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost Dataset 2\n",
        "5 Folds\n"
      ],
      "metadata": {
        "id": "j42vzpD4u-59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset2Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preparar los datos\n",
        "X = data.drop(columns=['Inputs'])  # Asumiendo que 'Outputs' es la variable objetivo\n",
        "y = data['Inputs']\n",
        "\n",
        "# Configurar el modelo XGBoost\n",
        "model = XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Configurar la validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir las métricas\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "scoring = {\n",
        "    'rmse': make_scorer(rmse, greater_is_better=False),\n",
        "    'r2': 'r2',\n",
        "    'mae': 'neg_mean_absolute_error'\n",
        "}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "# Resultados de la validación cruzada\n",
        "cv_results_mean = {metric: np.mean(scores) for metric, scores in cv_results.items()}\n",
        "cv_results_mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVBkvhvyvDyw",
        "outputId": "d0832f5d-50fa-4646-b4b4-ee169f50cda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': 2.6706839561462403,\n",
              " 'score_time': 0.24692630767822266,\n",
              " 'test_rmse': -25.408036092523126,\n",
              " 'test_r2': 0.4968255781861412,\n",
              " 'test_mae': -16.456384066572966}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost Dataset 2\n",
        "8 Folds\n"
      ],
      "metadata": {
        "id": "PdKduotlvbZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset2Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preparar los datos\n",
        "X = data.drop(columns=['Inputs'])  # Asumiendo que 'Outputs' es la variable objetivo\n",
        "y = data['Inputs']\n",
        "\n",
        "# Configurar el modelo XGBoost\n",
        "model = XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Configurar la validación cruzada\n",
        "kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir las métricas\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "scoring = {\n",
        "    'rmse': make_scorer(rmse, greater_is_better=False),\n",
        "    'r2': 'r2',\n",
        "    'mae': 'neg_mean_absolute_error'\n",
        "}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "# Resultados de la validación cruzada\n",
        "cv_results_mean = {metric: np.mean(scores) for metric, scores in cv_results.items()}\n",
        "cv_results_mean\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdBTpJncvXL_",
        "outputId": "f77577ac-f4df-416c-b0d7-609525c95e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage: 1473.42578125 megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQyx7l4bvfHj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kHvF_GN1U65e",
        "z25_I3fmYcSy",
        "aAXTAjREYmGC",
        "K2u4w2VPbCJR",
        "3gFzmVLXjkq1",
        "lm3_v8Hqj3sN",
        "Fhkp61TpkEr_",
        "4u07U7-hkOLU"
      ],
      "provenance": [],
      "mount_file_id": "1YsNBuAPj8-0dlZoJldgRDzxbkYV8jjI3",
      "authorship_tag": "ABX9TyPPT8ZiIyZ5EU3yDx54s5f7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}