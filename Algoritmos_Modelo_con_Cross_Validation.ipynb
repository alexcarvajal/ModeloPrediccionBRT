{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcarvajal/ModeloPrediccionBRT/blob/main/Algoritmos_Modelo_con_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de Datasets\n"
      ],
      "metadata": {
        "id": "UYswKk-LRMTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os, psutil\n",
        "url1 = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1Codificado.csv'\n",
        "data1 = pd.read_csv(url1)\n",
        "#data1 = data1[data1['Inputs'] != 0]\n",
        "data1\n",
        "url2 = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset2Codificado.csv'\n",
        "data2 = pd.read_csv(url2)\n",
        "#data2\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs3zi4OORLoy",
        "outputId": "d2a256ea-4f47-465c-a03f-42b7f97bf0ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage: 644.85546875 megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DoYrRgEUyI0"
      },
      "source": [
        "# Modelo Cross-Validation Dataset 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Dataset 1\n",
        "8 Folds"
      ],
      "metadata": {
        "id": "L5Dj1_AgYZPI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20df78f-d509-4da6-ccec-5e35f1c87c94",
        "id": "hXP2OjvNYLk2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE promedio: 619.4945218303951\n",
            "RMSE promedio: 24.610771902325478\n",
            "MAE promedio: 14.588860716088956\n",
            "R2 promedio: 0.27187781326887284\n",
            "MAPE promedio: inf\n",
            "Memory usage: 646.90234375 megabytes\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def adjusted_r2_score(y_true, y_pred, n, k):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Cargar el data1set\n",
        "\n",
        "# Eliminar columna innecesaria\n",
        "#data1 = data1.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Separar las características (X) de la variable objetivo (y)\n",
        "X = data1.drop(columns=['Inputs'])\n",
        "y = data1['Inputs']\n",
        "\n",
        "# Inicializar el modelo de Random Forest para regresión\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Definir las métricas para la validación cruzada\n",
        "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "           'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False),\n",
        "           'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "           'R2': 'r2',\n",
        "           'MAPE': make_scorer(mape, greater_is_better=False)}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(rf_regressor, X, y, cv=8, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
        "\n",
        "# Calcular y mostrar los resultados\n",
        "n, k = X.shape\n",
        "#r2_adj = adjusted_r2_score(y, rf_regressor.predict(X), n, k)\n",
        "\n",
        "print(f\"MSE promedio: {-cv_results['test_MSE'].mean()}\")\n",
        "print(f\"RMSE promedio: {-cv_results['test_RMSE'].mean()}\")\n",
        "print(f\"MAE promedio: {-cv_results['test_MAE'].mean()}\")\n",
        "print(f\"R2 promedio: {cv_results['test_R2'].mean()}\")\n",
        "#print(f\"R2 ajustado: {r2_adj}\")\n",
        "print(f\"MAPE promedio: {-cv_results['test_MAPE'].mean()}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE promedio: 492.63131760012857\n",
        "RMSE promedio: 22.178577349583172\n",
        "MAE promedio: 13.027049772057994\n",
        "R2 promedio: 0.4699380220836603\n",
        "MAPE promedio: inf\n",
        "Memory usage: 603.79296875 megabytes"
      ],
      "metadata": {
        "id": "iqydkBqtAv5Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHvF_GN1U65e"
      },
      "source": [
        "## Random Forest Dataset 1\n",
        "5 Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM_77zdDF9WK",
        "outputId": "251b0b40-3248-443d-df5f-978eccae849f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE promedio: 633.2430858609274\n",
            "RMSE promedio: 25.146622656948672\n",
            "MAE promedio: 14.885073642302126\n",
            "R2 promedio: 0.3146850858243407\n",
            "MAPE promedio: inf\n",
            "Memory usage: 452.67578125 megabytes\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def adjusted_r2_score(y_true, y_pred, n, k):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Cargar el data1set\n",
        "\n",
        "# Eliminar columna innecesaria\n",
        "#data1 = data1.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Separar las características (X) de la variable objetivo (y)\n",
        "X = data1.drop(columns=['Inputs'])\n",
        "y = data1['Inputs']\n",
        "\n",
        "# Inicializar el modelo de Random Forest para regresión\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Definir las métricas para la validación cruzada\n",
        "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "           'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False),\n",
        "           'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "           'R2': 'r2',\n",
        "           'MAPE': make_scorer(mape, greater_is_better=False)}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(rf_regressor, X, y, cv=5, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
        "\n",
        "# Calcular y mostrar los resultados\n",
        "n, k = X.shape\n",
        "#r2_adj = adjusted_r2_score(y, rf_regressor.predict(X), n, k)\n",
        "\n",
        "print(f\"MSE promedio: {-cv_results['test_MSE'].mean()}\")\n",
        "print(f\"RMSE promedio: {-cv_results['test_RMSE'].mean()}\")\n",
        "print(f\"MAE promedio: {-cv_results['test_MAE'].mean()}\")\n",
        "print(f\"R2 promedio: {cv_results['test_R2'].mean()}\")\n",
        "#print(f\"R2 ajustado: {r2_adj}\")\n",
        "print(f\"MAPE promedio: {-cv_results['test_MAPE'].mean()}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z25_I3fmYcSy"
      },
      "source": [
        "## LSTM  Dataset 1\n",
        "5 Folds\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY-PrZ2fnxcT",
        "outputId": "1a9c8ba6-d786-451d-d22e-9e99216e3145"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdZUhLBZGUuy",
        "outputId": "c81d253c-2a68-445c-940e-709a3f45770d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 820.757877017375, R2: 0.22200620100009893, MAPE: 1.4621422973820826e+16, RMSE: 28.648872177057424, mae: 19.00237056888953\n",
            "MSE: 872.9862971631319, R2: 0.2903574083836856, MAPE: 1.3005301778466942e+16, RMSE: 29.546341519097282, mae: 20.267478908047003\n",
            "MSE: 422.1852436890925, R2: 0.3392923516371309, MAPE: 8449993015053156.0, RMSE: 20.547146850331618, mae: 12.588068224382862\n",
            "MSE: 598.3023761327196, R2: 0.49076042580165735, MAPE: 9685021730678334.0, RMSE: 24.46022027972601, mae: 15.057741089388623\n",
            "MSE: 393.57230299519205, R2: 0.40721772326314243, MAPE: 7891112766099638.0, RMSE: 19.838656784046446, mae: 11.84030208062818\n",
            "Resumen de métricas:\n",
            "MSE promedio: 621.5608193995022\n",
            "R2 promedio: 0.34992682201714304\n",
            "MAPE promedio: 1.073057045282378e+16\n",
            "RMSE promedio: 24.60824752205176\n",
            "MAE promedio: 15.75119217426724\n",
            "Memory usage: 1221.734375 megabytes\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preprocesamiento\n",
        "features = data.drop(columns=['Inputs', 'Unnamed: 0'])\n",
        "target = data['Inputs']\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Convertir datos para LSTM\n",
        "X = np.reshape(features_scaled, (features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
        "y = target.values\n",
        "\n",
        "# Definir el modelo LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Callback para detener el entrenamiento\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
        "\n",
        "# TimeSeriesSplit para la validación cruzada\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "metrics_summary = {'mse': [], 'r2': [], 'mape': [], 'rmse':[], 'mae':[]}\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=32, verbose=0)\n",
        "\n",
        "    # Predecir\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "    # Calcular métricas\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
        "    #r2_adjusted = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "    # Guardar métricas\n",
        "    metrics_summary['mse'].append(mse)\n",
        "    metrics_summary['r2'].append(r2)\n",
        "    metrics_summary['mape'].append(mape)\n",
        "    metrics_summary['rmse'].append(rmse)\n",
        "    metrics_summary['mae'].append(mae)\n",
        "\n",
        "\n",
        "    print(f'MSE: {mse}, R2: {r2}, MAPE: {mape}, RMSE: {rmse}, mae: {mae}')\n",
        "\n",
        "# Mostrar resumen de métricas\n",
        "print(\"Resumen de métricas:\")\n",
        "print(f\"MSE promedio: {np.mean(metrics_summary['mse'])}\")\n",
        "print(f\"R2 promedio: {np.mean(metrics_summary['r2'])}\")\n",
        "print(f\"MAPE promedio: {np.mean(metrics_summary['mape'])}\")\n",
        "print(f\"RMSE promedio: {np.mean(metrics_summary['rmse'])}\")\n",
        "print(f\"MAE promedio: {np.mean(metrics_summary['mae'])}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM  Dataset 1\n",
        "8 Folds\n"
      ],
      "metadata": {
        "id": "I7xAQoyWwhWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1d89f3-0062-42b0-8c8b-561399c5ee0f",
        "id": "R9djDKQ7wXVn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 933.3507567461394, R2: 0.19436474084681132, MAPE: 1.7470643370034908e+16, RMSE: 30.550789789236863, mae: 21.349915690855216\n",
            "MSE: 797.7529811163081, R2: 0.2003173860375338, MAPE: 1.7424354569654416e+16, RMSE: 28.244521258401743, mae: 19.231364633393735\n",
            "MSE: 971.1821067919277, R2: 0.2383405590068859, MAPE: 1.4459741195918386e+16, RMSE: 31.163794807306886, mae: 21.08385106786063\n",
            "MSE: 655.3658177044389, R2: 0.4287978401805136, MAPE: 1.038775748976248e+16, RMSE: 25.600113626787653, mae: 15.956248173788433\n",
            "MSE: 252.02708095366035, R2: 0.2559342666745893, MAPE: 9123224274381312.0, RMSE: 15.875360813337766, mae: 10.416769936854884\n",
            "MSE: 601.7775407253674, R2: 0.49892004465552386, MAPE: 1.0971822498835008e+16, RMSE: 24.531154492305646, mae: 15.46956352460399\n",
            "MSE: 605.5616744610766, R2: 0.46959591764622977, MAPE: 1.1335912934923142e+16, RMSE: 24.60816276078075, mae: 15.879088313334615\n",
            "MSE: 249.3937308431027, R2: 0.3451479533519929, MAPE: 7200897594502959.0, RMSE: 15.792204749277497, mae: 9.730678852617107\n",
            "Resumen de métricas:\n",
            "MSE promedio: 633.3014611677527\n",
            "R2 promedio: 0.32892733855001005\n",
            "MAPE promedio: 1.2296794241001576e+16\n",
            "RMSE promedio: 24.54576278717935\n",
            "MAE promedio: 16.139685024163576\n",
            "Memory usage: 1249.6796875 megabytes\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preprocesamiento\n",
        "features = data.drop(columns=['Inputs', 'Unnamed: 0'])\n",
        "target = data['Inputs']\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Convertir datos para LSTM\n",
        "X = np.reshape(features_scaled, (features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
        "y = target.values\n",
        "\n",
        "# Definir el modelo LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Callback para detener el entrenamiento\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
        "\n",
        "# TimeSeriesSplit para la validación cruzada\n",
        "tscv = TimeSeriesSplit(n_splits=8)\n",
        "\n",
        "metrics_summary = {'mse': [], 'r2': [], 'mape': [], 'rmse':[], 'mae':[]}\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=32, verbose=0)\n",
        "\n",
        "    # Predecir\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "    # Calcular métricas\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
        "    #r2_adjusted = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "    # Guardar métricas\n",
        "    metrics_summary['mse'].append(mse)\n",
        "    metrics_summary['r2'].append(r2)\n",
        "    metrics_summary['mape'].append(mape)\n",
        "    metrics_summary['rmse'].append(rmse)\n",
        "    metrics_summary['mae'].append(mae)\n",
        "\n",
        "\n",
        "    print(f'MSE: {mse}, R2: {r2}, MAPE: {mape}, RMSE: {rmse}, mae: {mae}')\n",
        "\n",
        "# Mostrar resumen de métricas\n",
        "print(\"Resumen de métricas:\")\n",
        "print(f\"MSE promedio: {np.mean(metrics_summary['mse'])}\")\n",
        "print(f\"R2 promedio: {np.mean(metrics_summary['r2'])}\")\n",
        "print(f\"MAPE promedio: {np.mean(metrics_summary['mape'])}\")\n",
        "print(f\"RMSE promedio: {np.mean(metrics_summary['rmse'])}\")\n",
        "print(f\"MAE promedio: {np.mean(metrics_summary['mae'])}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAXTAjREYmGC"
      },
      "source": [
        "## SVM  Dataset 1\n",
        "70% Entrenamiento - 30% Prueba\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tULxh3TlJgAT",
        "outputId": "b84f338e-5479-455f-bd42-9e799ed30aa3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "\n",
        "# Preprocesamiento básico\n",
        "# Eliminar columna innecesaria y separar características de la variable objetivo\n",
        "X = data.drop(columns=['Inputs', 'Unnamed: 0'])\n",
        "y = data['Inputs']\n",
        "\n",
        "# Crear un pipeline que primero estandariza los datos y luego aplica SVM\n",
        "svm_pipeline = make_pipeline(StandardScaler(), SVC(kernel='linear', random_state=42))\n",
        "\n",
        "# Aplicar validación cruzada\n",
        "cv_scores = cross_val_score(svm_pipeline, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Mostrar los resultados de la validación cruzada\n",
        "print(f\"Accuracy promedio: {cv_scores.mean()}\")\n",
        "print(f\"Desviación estándar: {cv_scores.std()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmYlav2HY4NQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2u4w2VPbCJR"
      },
      "source": [
        "## XGBoost Dataset 1\n",
        "5 Folds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTP4yETfbGlG",
        "outputId": "a5c60f7c-ab8f-4a4f-b3ea-69246c561225"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': 4.855373668670654,\n",
              " 'score_time': 0.3214241027832031,\n",
              " 'test_rmse': -20.918190008871687,\n",
              " 'test_r2': 0.5352016323592881,\n",
              " 'test_mae': -12.392111442769579}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preparar los datos\n",
        "X = data.drop(columns=['Inputs', 'Unnamed: 0'])  # Asumiendo que 'Outputs' es la variable objetivo\n",
        "y = data['Inputs']\n",
        "\n",
        "# Configurar el modelo XGBoost\n",
        "model = XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Configurar la validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir las métricas\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "scoring = {\n",
        "    'rmse': make_scorer(rmse, greater_is_better=False),\n",
        "    'r2': 'r2',\n",
        "    'mae': 'neg_mean_absolute_error'\n",
        "}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "# Resultados de la validación cruzada\n",
        "cv_results_mean = {metric: np.mean(scores) for metric, scores in cv_results.items()}\n",
        "cv_results_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQyeLmGjbqQq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIROuvzSk_5J"
      },
      "source": [
        "## XGBoost Dataset 1\n",
        "8 Folds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2b5ce9-a3ba-4154-81b1-3ee0def29035",
        "id": "Qg3PN-XWk_5P"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fit_time': 6.065488517284393, 'score_time': 0.25341299176216125, 'test_rmse': -20.90517851139823, 'test_r2': 0.5357589932300635, 'test_mae': -12.380066040782962}\n",
            "Memory usage: 1511.47265625 megabytes\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset1Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preparar los datos\n",
        "X = data.drop(columns=['Inputs', 'Unnamed: 0'])  # Asumiendo que 'Outputs' es la variable objetivo\n",
        "y = data['Inputs']\n",
        "\n",
        "# Configurar el modelo XGBoost\n",
        "model = XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Configurar la validación cruzada\n",
        "kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir las métricas\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "scoring = {\n",
        "    'rmse': make_scorer(rmse, greater_is_better=False),\n",
        "    'r2': 'r2',\n",
        "    'mae': 'neg_mean_absolute_error'\n",
        "}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "# Resultados de la validación cruzada\n",
        "cv_results_mean = {metric: np.mean(scores) for metric, scores in cv_results.items()}\n",
        "print(cv_results_mean)\n",
        "\n",
        "process = psutil.Process()\n",
        "\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZZplr6Dk_5Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo con Cross Validation Dataset 2"
      ],
      "metadata": {
        "id": "2SNf7efzlgja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Dataset 2\n",
        "8 Folds\n"
      ],
      "metadata": {
        "id": "DLoVk-HIlxNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def adjusted_r2_score(y_true, y_pred, n, k):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Cargar el data2set\n",
        "\n",
        "# Eliminar columna innecesaria\n",
        "#data2 = data1.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Separar las características (X) de la variable objetivo (y)\n",
        "X = data2.drop(columns=['Inputs'])\n",
        "y = data2['Inputs']\n",
        "\n",
        "# Inicializar el modelo de Random Forest para regresión\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Definir las métricas para la validación cruzada\n",
        "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "           'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False),\n",
        "           'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "           'R2': 'r2',\n",
        "           'MAPE': make_scorer(mape, greater_is_better=False)}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(rf_regressor, X, y, cv=8, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
        "\n",
        "# Calcular y mostrar los resultados\n",
        "n, k = X.shape\n",
        "#r2_adj = adjusted_r2_score(y, rf_regressor.predict(X), n, k)\n",
        "\n",
        "print(f\"MSE promedio: {-cv_results['test_MSE'].mean()}\")\n",
        "print(f\"RMSE promedio: {-cv_results['test_RMSE'].mean()}\")\n",
        "print(f\"MAE promedio: {-cv_results['test_MAE'].mean()}\")\n",
        "print(f\"R2 promedio: {cv_results['test_R2'].mean()}\")\n",
        "#print(f\"R2 ajustado: {r2_adj}\")\n",
        "print(f\"MAPE promedio: {-cv_results['test_MAPE'].mean()}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbpkClfLl4d4",
        "outputId": "147e23bf-3025-43c0-cdfb-c4a38a82f261"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE promedio: 701.1035907524173\n",
            "RMSE promedio: 26.089159767388324\n",
            "MAE promedio: 16.9907470936229\n",
            "R2 promedio: 0.40982396761203843\n",
            "MAPE promedio: 303.96586669293475\n",
            "Memory usage: 1511.57421875 megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Random Forest Dataset 2\n",
        "5 Folds\n",
        "\n"
      ],
      "metadata": {
        "id": "J58vQFXvmO-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def adjusted_r2_score(y_true, y_pred, n, k):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Cargar el data2set\n",
        "\n",
        "# Eliminar columna innecesaria\n",
        "#data2 = data1.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Separar las características (X) de la variable objetivo (y)\n",
        "X = data2.drop(columns=['Inputs'])\n",
        "y = data2['Inputs']\n",
        "\n",
        "# Inicializar el modelo de Random Forest para regresión\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Definir las métricas para la validación cruzada\n",
        "scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "           'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False),\n",
        "           'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "           'R2': 'r2',\n",
        "           'MAPE': make_scorer(mape, greater_is_better=False)}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(rf_regressor, X, y, cv=5, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
        "\n",
        "# Calcular y mostrar los resultados\n",
        "n, k = X.shape\n",
        "#r2_adj = adjusted_r2_score(y, rf_regressor.predict(X), n, k)\n",
        "\n",
        "print(f\"MSE promedio: {-cv_results['test_MSE'].mean()}\")\n",
        "print(f\"RMSE promedio: {-cv_results['test_RMSE'].mean()}\")\n",
        "print(f\"MAE promedio: {-cv_results['test_MAE'].mean()}\")\n",
        "print(f\"R2 promedio: {cv_results['test_R2'].mean()}\")\n",
        "#print(f\"R2 ajustado: {r2_adj}\")\n",
        "print(f\"MAPE promedio: {-cv_results['test_MAPE'].mean()}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS4ekcObmLzB",
        "outputId": "f83e9d7e-933b-4867-81c5-ba924abfc717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE promedio: 699.9964771596826\n",
            "RMSE promedio: 26.34454554438351\n",
            "MAE promedio: 16.964475047524537\n",
            "R2 promedio: 0.452893748350753\n",
            "MAPE promedio: 302.27969871179414\n",
            "Memory usage: 1679.8046875 megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Dataset 2\n",
        "8 Folds"
      ],
      "metadata": {
        "id": "pf5_wSbUnL2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset2Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preprocesamiento\n",
        "features = data.drop(columns=['Inputs'])\n",
        "target = data['Inputs']\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Convertir datos para LSTM\n",
        "X = np.reshape(features_scaled, (features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
        "y = target.values\n",
        "\n",
        "# Definir el modelo LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Callback para detener el entrenamiento\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
        "\n",
        "# TimeSeriesSplit para la validación cruzada\n",
        "tscv = TimeSeriesSplit(n_splits=8)\n",
        "\n",
        "metrics_summary = {'mse': [], 'r2': [], 'mape': [], 'rmse':[], 'mae':[]}\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=32, verbose=0)\n",
        "\n",
        "    # Predecir\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "    # Calcular métricas\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
        "    #r2_adjusted = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "    # Guardar métricas\n",
        "    metrics_summary['mse'].append(mse)\n",
        "    metrics_summary['r2'].append(r2)\n",
        "    metrics_summary['mape'].append(mape)\n",
        "    metrics_summary['rmse'].append(rmse)\n",
        "    metrics_summary['mae'].append(mae)\n",
        "\n",
        "\n",
        "    print(f'MSE: {mse}, R2: {r2}, MAPE: {mape}, RMSE: {rmse}, mae: {mae}')\n",
        "\n",
        "# Mostrar resumen de métricas\n",
        "print(\"Resumen de métricas:\")\n",
        "print(f\"MSE promedio: {np.mean(metrics_summary['mse'])}\")\n",
        "print(f\"R2 promedio: {np.mean(metrics_summary['r2'])}\")\n",
        "print(f\"MAPE promedio: {np.mean(metrics_summary['mape'])}\")\n",
        "print(f\"RMSE promedio: {np.mean(metrics_summary['rmse'])}\")\n",
        "print(f\"MAE promedio: {np.mean(metrics_summary['mae'])}\")\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6xA2Sm4nRRo",
        "outputId": "4c5401f7-09ee-46b6-8d35-dcefc9e5f0f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1432.3084327418048, R2: 0.09809159919588373, MAPE: 6.020938892115022, RMSE: 37.84585093166495, mae: 28.05084610263529\n",
            "MSE: 941.4976058709402, R2: 0.11807365902234301, MAPE: 4.6684390009145975, RMSE: 30.683832972282655, mae: 21.79927126566246\n",
            "MSE: 2214.9753676513096, R2: -0.1085475496633812, MAPE: 3.985925857484152, RMSE: 47.063524811166765, mae: 34.20114958282731\n",
            "MSE: 945.9925605943497, R2: 0.3856651797461804, MAPE: 3.14592709784236, RMSE: 30.756992060251108, mae: 20.714166038006688\n",
            "MSE: 461.2409618755735, R2: 0.18657783170110298, MAPE: 2.9116982086592404, RMSE: 21.47652117721987, mae: 14.36691632580847\n",
            "MSE: 830.8500093251808, R2: 0.446461904214444, MAPE: 4.185714551916844, RMSE: 28.82446893396617, mae: 20.01817676064173\n",
            "MSE: 848.7254204941432, R2: 0.4422560687039374, MAPE: 3.390253351063788, RMSE: 29.13289241551795, mae: 19.628032216312512\n",
            "MSE: 392.3433761390404, R2: 0.3195353783746484, MAPE: 2.581009632540265, RMSE: 19.807659532086078, mae: 12.96859996156046\n",
            "Resumen de métricas:\n",
            "MSE promedio: 1008.4917168365427\n",
            "R2 promedio: 0.23601425891189481\n",
            "MAPE promedio: 3.8612383240670334\n",
            "RMSE promedio: 30.69896785426944\n",
            "MAE promedio: 21.468394781681866\n",
            "Memory usage: 1390.85546875 megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Dataset 2\n",
        "5 Folds"
      ],
      "metadata": {
        "id": "p_nL2sL0nte3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset2Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preprocesamiento\n",
        "features = data.drop(columns=['Inputs'])\n",
        "target = data['Inputs']\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Convertir datos para LSTM\n",
        "X = np.reshape(features_scaled, (features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
        "y = target.values\n",
        "\n",
        "# Definir el modelo LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Callback para detener el entrenamiento\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
        "\n",
        "# TimeSeriesSplit para la validación cruzada\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "metrics_summary = {'mse': [], 'r2': [], 'mape': [], 'rmse':[], 'mae':[]}\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=32, verbose=0)\n",
        "\n",
        "    # Predecir\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "    # Calcular métricas\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
        "    #r2_adjusted = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "    # Guardar métricas\n",
        "    metrics_summary['mse'].append(mse)\n",
        "    metrics_summary['r2'].append(r2)\n",
        "    metrics_summary['mape'].append(mape)\n",
        "    metrics_summary['rmse'].append(rmse)\n",
        "    metrics_summary['mae'].append(mae)\n",
        "\n",
        "\n",
        "    print(f'MSE: {mse}, R2: {r2}, MAPE: {mape}, RMSE: {rmse}, mae: {mae}')\n",
        "\n",
        "# Mostrar resumen de métricas\n",
        "print(\"Resumen de métricas:\")\n",
        "print(f\"MSE promedio: {np.mean(metrics_summary['mse'])}\")\n",
        "print(f\"R2 promedio: {np.mean(metrics_summary['r2'])}\")\n",
        "print(f\"MAPE promedio: {np.mean(metrics_summary['mape'])}\")\n",
        "print(f\"RMSE promedio: {np.mean(metrics_summary['rmse'])}\")\n",
        "print(f\"MAE promedio: {np.mean(metrics_summary['mae'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_reZyEblnw8l",
        "outputId": "cc9e3d1f-4007-4674-f3d3-f953545c12fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1017.4893373191373, R2: 0.18393345926438265, MAPE: 4.739793689079243, RMSE: 31.898108679342375, mae: 22.175716125722374\n",
            "MSE: 1708.3603230424992, R2: 0.08619616532230401, MAPE: 3.40859850164396, RMSE: 41.332315723202576, mae: 27.919703004179098\n",
            "MSE: 668.7079956718953, R2: 0.25822865311654153, MAPE: 2.7259120380313786, RMSE: 25.85938892688486, mae: 17.031401384371335\n",
            "MSE: 1191.0483707961496, R2: 0.21828234645706002, MAPE: 4.104327432980178, RMSE: 34.511568651629695, mae: 23.46774439230608\n",
            "MSE: 555.16777279784, R2: 0.3910135174437739, MAPE: 2.7707685736590517, RMSE: 23.561998489046722, mae: 15.179332963280393\n",
            "Resumen de métricas:\n",
            "MSE promedio: 1028.1547599255043\n",
            "R2 promedio: 0.22753082832081245\n",
            "MAPE promedio: 3.549880047078762\n",
            "RMSE promedio: 31.432676094021247\n",
            "MAE promedio: 21.154779573971854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost Dataset 2\n",
        "5 Folds\n"
      ],
      "metadata": {
        "id": "j42vzpD4u-59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset2Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preparar los datos\n",
        "X = data.drop(columns=['Inputs'])  # Asumiendo que 'Outputs' es la variable objetivo\n",
        "y = data['Inputs']\n",
        "\n",
        "# Configurar el modelo XGBoost\n",
        "model = XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Configurar la validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir las métricas\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "scoring = {\n",
        "    'rmse': make_scorer(rmse, greater_is_better=False),\n",
        "    'r2': 'r2',\n",
        "    'mae': 'neg_mean_absolute_error'\n",
        "}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "# Resultados de la validación cruzada\n",
        "cv_results_mean = {metric: np.mean(scores) for metric, scores in cv_results.items()}\n",
        "cv_results_mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVBkvhvyvDyw",
        "outputId": "d0832f5d-50fa-4646-b4b4-ee169f50cda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': 2.6706839561462403,\n",
              " 'score_time': 0.24692630767822266,\n",
              " 'test_rmse': -25.408036092523126,\n",
              " 'test_r2': 0.4968255781861412,\n",
              " 'test_mae': -16.456384066572966}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost Dataset 2\n",
        "8 Folds\n"
      ],
      "metadata": {
        "id": "PdKduotlvbZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Cargar datos\n",
        "dataset_path = '/content/drive/MyDrive/Reuniones/DatasetsWP2/Dataset2Codificado.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preparar los datos\n",
        "X = data.drop(columns=['Inputs'])  # Asumiendo que 'Outputs' es la variable objetivo\n",
        "y = data['Inputs']\n",
        "\n",
        "# Configurar el modelo XGBoost\n",
        "model = XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Configurar la validación cruzada\n",
        "kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir las métricas\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "scoring = {\n",
        "    'rmse': make_scorer(rmse, greater_is_better=False),\n",
        "    'r2': 'r2',\n",
        "    'mae': 'neg_mean_absolute_error'\n",
        "}\n",
        "\n",
        "# Aplicar la validación cruzada\n",
        "cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "# Resultados de la validación cruzada\n",
        "cv_results_mean = {metric: np.mean(scores) for metric, scores in cv_results.items()}\n",
        "cv_results_mean\n",
        "process = psutil.Process()\n",
        "print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024)} megabytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdBTpJncvXL_",
        "outputId": "f77577ac-f4df-416c-b0d7-609525c95e05"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage: 1473.42578125 megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQyx7l4bvfHj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kHvF_GN1U65e",
        "z25_I3fmYcSy",
        "aAXTAjREYmGC",
        "K2u4w2VPbCJR",
        "3gFzmVLXjkq1",
        "lm3_v8Hqj3sN",
        "Fhkp61TpkEr_",
        "4u07U7-hkOLU"
      ],
      "provenance": [],
      "mount_file_id": "1YsNBuAPj8-0dlZoJldgRDzxbkYV8jjI3",
      "authorship_tag": "ABX9TyPxc4aAgApCsr41kYOwkMj7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}